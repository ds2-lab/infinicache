1) Multikey with LowLevelKeys Replication Operations

    EXPERIMENT (1.1)
    - 500 Consecutive mkSET requests followed by 500 Consecutive rGET requests -
    - low-level-value-size = 160 B (the same as the average size of Fraudio's data)
    - for each SET we set a  high-level-key which contains 9 low-level-keys (1404 B in total)
    - for each GET we retrieve 3 low-level key-value pairs (480 B)
    Average mkSET time:  5.381147e+06
    Average mkGET time:  6.844392e+07

    EXPERIMENT (1.2)
    - 1000 Consecutive mkSET requests followed by 1000 Consecutive rGET requests -
    - low-level-value-size = 160 B (the same as the average size of Fraudio's data)
    - for each SET we set a  high-level-key which contains 9 low-level-keys (1404 B in total)
    - for each GET we retrieve 3 low-level key-value pairs (480 B)
    Average mkSET time:  4.9947485e+06
    Average mkGET time:  2.1112165e+08

    EXPERIMENT (1.3)
    - same as 1.2 but with low-level-value-size = 1313 B which is the max value for Fraudio's Data
    Average mkSET time:  7.928871052999994
    Average mkGET time:  245.823701719

    EXPERIMENT (1.4)
    - we launch 3 clients which perform concurrently 1000 mkSET each against the same proxy
    - low-level-value-size = 1313 B
    - we then take the average of the averages of each client as the average time
    Average mkSET time:  9.49622103


2) Simple Replication Operations

    EXPERIMENT (2.1)
    - 500 Consecutive rSET requests followed by 500 Consecutive rGET requests -
    - low-level-value-size = 2 Bytes (first test)
    Average rSET time:  4.516564e+06 ms
    Average rGET time:  2.570904e+06 ms

    EXPERIMENT (2.2)
    - 1000 Consecutive rSET requests followed by 1000 Consecutive rGET requests -
    - low-level-value-size = 160 B (the same as the average size of Fraudio's data)
    Average mkSET time:  4.3145145e+06
    Average mkGET time:  2.387181e+06

3) Original Erasure Coding Operations

    EXPERIMENT (3.1)
    - 500 Consecutive SET requests followed by 500 Consecutive GET requests -
    - value-size = 160 Bytes
    Average SET time:  6.676049121756492 ns
    Average GET time:  159.64132920758456 ms

    EXPERIMENT (3.2)
    - 1000 Consecutive SET requests followed by 1000 Consecutive GET requests -
    - value size = 160 Bytes
    Average SET time:  6.410645341658353 ms
    Average GET time:  169.11462692007993 ms


    EXPERIMENT (3.3)
    - tries to simulate the Multikey operation
    - basically we assume that every time we need to save data into the cache
    we need to SET 9 key-value pairs (we replicate the Multi Key operation)
    - then when we need to retrieve data, we need to GET 3 key-value pairs (480 B)
    Average GET time:  500.924516480519 ms
    Average SET time:  62.18107363036967 ms

    EXPERIMENT (3.4)
    - 3 clients concurrently SETting and GETting different keys into the system
    - the value of each kv pair is the maximum value from Fraudio's data 1313 B
    - we basically launch 3 contemporan go script which represent the clients and
    then take the average of their averages as the average of the concurrent calls
    Average SET time:  105.84354689310699 ms
    Average GET time:  1559.4027978676472 ms

    OBSERVATION:
    When we perform many operations against infinicache classical version we observe
    that many chunks are lost and then many of them have to be recovered, which obviously
    increases the latency and deteriorates performance. This may happen because of the
    large number of connections that the proxy has to manage and on too many concurrent
    calls many of the packets may get lost and therefore there is the need for recovery

    Next:
    - concurrent with 1 proxy also for R and MK
    - experiments with more proxies, concurrent and consecutive
    - run real workload and analyze logs
    - test other configurations, different number of replicas
    - try simple replication for all data, without low-level-keys,
    but retrieve concurrently multiple keys, based on how many keys
    the Scoring API needs
    - understand how the number of connections is related to economical
    costs in the new setups
    - evaluate to modify infinicache in a way is trully elastic using
    the serverless features of knative; we could basically organize
    the nodes pool of each proxy using consistent hashing and trigger
    a new lambda, whenever the space becomes limitate but the requests
    rate still grows; of course whenever a new node joins a migration
    would be required

